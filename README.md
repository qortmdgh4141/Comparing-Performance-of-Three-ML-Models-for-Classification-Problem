# ğŸ¾  Comparing Performance of Three ML Models for Classification Problem   
<br/> 
  
### 1. &nbsp; Research Objective <br/><br/>

- _The wine dataset contains the results of a chemical analysis of wines grown in a specific area of Italy. Three types of wine are represented in the 178 samples, with the results of 13 chemical analyses recorded for each sample. The Type variable has been transformed into a categoric variable. The data contains no missing values and consits of only numeric data, with a three class target variable (Type) for classification._ <br/>

- _The goal here is to find a model that can predict the class of wine given the 13 measured parameters and find out the major differences among the three different classes. This is a classification problem and here I will describe three models and asses the accuracy of each model._ <br/><br/><br/> 

### 2. &nbsp; Data Preprocessing and Analysis <br/><br/>

- _**Package Settings**_ <br/> 
  
  ```
  # sklearn íŒ¨í‚¤ì§€ì—ì„œ ì™€ì¸ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ load_wine ì„¤ì •
  # í‘œì¤€í™”ë¥¼ ìœ„í•´ StandaradScaler ì„¤ì •
  # í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬ë¥¼ ìœ„í•´ train_test_split ì„¤ì •
  from sklearn.datasets import load_wine
  from sklearn.preprocessing import StandardScaler
  from sklearn.model_selection import train_test_split
  import pandas as pd
  import matplotlib.pyplot as plt

  # ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •
  import numpy as np
  from matplotlib import cm
  from matplotlib.colors import ListedColormap, LinearSegmentedColormap
  from matplotlib import colors as mcolors, path
  ```
  
- _**Data Preparation**_ <br/> 

  ```
  # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
  data = load_wine(as_frame = True)
  
  # ë°ì´í„° í”„ë ˆì„ ì¶œë ¥ 
  # ë°ì´í„°ë¥¼ ì¶œë ¥í•¨ìœ¼ë¡œì¨ ê°œëµì ì¸ ìˆ˜ì¹˜ë¥¼ í™•ì¸ 
  print(data.frame)
  
  # ì…ë ¥ ë¶€ë¶„ê³¼ ëª©í‘œ ê°’ì„ ì¶œë ¥ 
  # ë°ì´í„°ì˜ ì…ë ¥ ë¶€ë¶„ê³¼ ëª©í‘œ ë³€ìˆ˜ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì¶œë ¥í•¨ìœ¼ë¡œì¨ ê°œëµì ì¸ ìˆ˜ì¹˜ë¥¼ í™•ì¸ 
  print(data.data)
  print(data.target)
  ```
  
- _**Exploratory Data Analysis (EDA)**_ <br/> 

  ```
  # ë°ì´í„° ì„¸íŠ¸ ê°œìš”
  # ê° ì—´ë³„ ìµœëŒ€ ë° ìµœì†Œ ë¶„í¬ë¥¼ íŒŒì•…
  print(data.DESCR)
  
  # ë°ì´í„° í”„ë ˆì„ì˜ íŠ¹ì„±(feature) ì´ë¦„ê³¼ ëª©í‘œ ë³€ìˆ˜(target) ì´ë¦„
  # ëª©í‘œ ë³€ìˆ˜ì—ì„œ ìˆœì„œëŒ€ë¡œ 'class_0'ëŠ” 0, 'class_1'ì€ 1, 'class_2'ì€ 2ì„ ì˜ë¯¸
  print("[ ë°ì´í„° í”„ë ˆì„ì˜ íŠ¹ì„±(feature) ]")
  for feature in data.feature_names:
      print(f' - {feature}')
  print("\n [ ë°ì´í„° í”„ë ˆì„ì˜ ëª©í‘œ ë³€ìˆ˜(target) ]")
  for target in data.target_names:
      print(f' - {target}')
  ``` 

- _**Splitting Data**_ <br/> 

  ```
  # random_state = ìì‹ ì˜ ìƒì¼ë¡œ ì„¤ì • => 07ì›” 23ì¼ => 0723 => 723
  # data ë¶€ë¶„ì„ 8:2 ë¹„ìœ¨ë¡œ x_trainê³¼ x_testë¡œ ë‚˜ëˆ”
  # target ë¶€ë¶„ì„ 8:2 ë¹„ìœ¨ë¡œ y_trainê³¼ y_testë¡œ ë‚˜ëˆ”
  # í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
  x_train, x_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=723)
  ```  
  
- _**Feature Scaling**_ <br/> 
  ```
  # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ : í•™ìŠµ ë°ì´í„° 
  # í•™ìŠµ ë°ì´í„°ì˜ ì…ë ¥ ë°ì´í„°ë¥¼ ê° ì—´ë³„ë¡œ í‘œì¤€í™”
  scaler_x = StandardScaler()
  scaler_x.fit(x_train)
  x_train_std = scaler_x.transform(x_train)

  # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ : í…ŒìŠ¤íŠ¸ ë°ì´í„° 
  # í•™ìŠµ ë°ì´í„°ì˜ í‘œì¤€í™” ìŠ¤ì¼€ì¼ì„ ì‚¬ìš©í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í‘œì¤€í™”
  x_test_std = scaler_x.transform(x_test)
  ``` 
  <br/> 

### 3. &nbsp; Training and Testing Machine Learning Models <br/><br/>

- _**KNN Model**_ <br/> 
  - _KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification)._ <br/><br/>
  
  ```
  # KNN ë¶„ë¥˜ë¥¼ ìœ„í•´ KNeighborsClassifier ì„¤ì •
  from sklearn.neighbors import KNeighborsClassifier

  ## ìµœê·¼ì ‘ ì´ì›ƒ ìˆ˜ ê²°ì •
  # í•™ìŠµ & í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¶„ë¥˜ ì •í™•ë„
  knn_train_accuracy = []
  knn_test_accuracy = []
  
  # ìµœê·¼ì ‘ ì´ì›ƒì˜ ìˆ˜(k) 
  # -> kê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆ ì„±ë¶„ê¹Œì§€ ê³ ë ¤í•˜ëŠ” ê³¼ëŒ€ì í•©(overfitting) ë¬¸ì œê°€ ë°œìƒ, 
  # -> ë°˜ëŒ€ë¡œ kë¥¼ ë„ˆë¬´ í¬ê²Œ í•˜ë©´ ê²°ì •í•¨ìˆ˜ê°€ ë„ˆë¬´ ê³¼í•˜ê²Œ í‰íƒ„í™”(oversmoothing)ë˜ëŠ” ë¬¸ì œê°€ ë°œìƒ
  # -> ìµœì ì˜ K ê°’ì„ ì°¾ëŠ”ê²ƒì´ ì¤‘ìš”
  # num_neighbors : ìµœì ì˜ ìµœê·¼ì ‘ ì´ì›ƒì˜ ìˆ˜(k)ì„ ì°¾ê¸° ìœ„í•œ í›„ë³´ ìˆ«ì
  # kë¥¼ 1~101ì˜ ë²”ìœ„ì—ì„œ ì°¾ì•„ë´„ 
  num_neighbors = range(1, 101)
  for k in num_neighbors:
      # ëª¨í˜•í™”
      # ìœ ì‚¬ë„ ì¸¡ì • ì§€í‘œ(metric)ì€ ìœ í´ë¦¬ë“œ ê±°ë¦¬, ë§¨í•˜íƒ„ ê±°ë¦¬, ë¯¼ì½”ë¸ŒìŠ¤í‚¨ ê±°ë¦¬ ì¤‘ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¡œ ì„ ì •
      knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean')
      # í•™ìŠµ
      knn.fit(x_train_std, y_train)
      # í•™ìŠµ ë°ì´í„°ì˜ ë¶„ë¥˜ ì •í™•ë„
      score = knn.score(x_train_std, y_train) # í•™ìŠµ ëª¨í˜•ì— ì˜í•œ í•™ìŠµ ë°ì´í„°ì˜ ë¶„ë¥˜ ì •í™•ë„ ê°’
      knn_train_accuracy.append(score) # í›„ë³´ k ê°’ì— ë”°ë¥¸ í•™ìŠµ ë°ì´í„°ì˜ ë¶„ë¥˜ ì •í™•ë„ ê°’ ì¶”ê°€
      # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¶„ë¥˜ ì •í™•ë„
      score = knn.score(x_test_std, y_test) # í•™ìŠµ ëª¨í˜•ì— ì˜í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¶„ë¥˜ ì •í™•ë„ ê°’
      knn_test_accuracy.append(score) # í›„ë³´ k ê°’ì— ë”°ë¥¸ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ë¶„ë¥˜ ì •í™•ë„ ê°’ ì¶”ê°€

  # í›„ë³´ kì˜ ê°’ì— ë”°ë¥¸ ì •í™•ë„ë¥¼ ë¹„êµ
  # k ê°’ì´ 80ë³´ë‹¤ ì»¤ì§ˆ ìˆ˜ë¡ ê³¼ëŒ€ì í•©(overfitting)&í‰íƒ„í™”(oversmoothing) ë¬¸ì œ ë“±ì´ ë°œìƒ!
  # ìµœì ì˜ k ê°’ì€ 5ë¡œ ì„ ì •!
  max_accuracy = 0
  for num_k, accuracy in enumerate(knn_test_accuracy):
      if max_accuracy < accuracy :
          max_accuracy = accuracy
          best_k = num_k+1
  print(f"ìµœì ì˜ k ê°’ : {best_k}")  
  print(f'ìµœì ì˜ k ê°’ì˜ ì •í™•ë„ : {max_accuracy} \n') 

  plt.plot(num_neighbors, knn_train_accuracy, label="train")
  plt.plot(num_neighbors, knn_test_accuracy, label="test")
  plt.xlabel("K")
  plt.ylabel("Accuracy")
  plt.legend()
  plt.show()
  ```  
  <img src="https://github.com/qortmdgh4141/Classifying_Wines_by_Quality_Using_Machine_Learning/blob/main/image/line_graph.png?raw=true"  width="640" > <br/>
  ```
  # KNN ë¶„ë¥˜ë¥¼ ìœ„í•´ KNeighborsClassifier ì„¤ì •
  from sklearn.neighbors import KNeighborsClassifier
  # ëª¨í˜•í™” : këŠ” 5ë¡œ ì„¤ì •
  knn = KNeighborsClassifier(n_neighbors=k)
  # í•™ìŠµ
  knn.fit(x_train_std, y_train)
  ```
  ```
  # KNN í•™ìŠµëœ ëª¨í˜•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ë¥˜í–ˆì„ë•Œì˜ ì •í™•ë„ (kë¥¼ 5ë¡œ ì„¤ì •)
  knn_test_acuaracy = knn.score(x_test_std, y_test)
  knn_result = round(knn_test_acuaracy*100, 2)
  print(f'KNN ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ë¶„ë¥˜ ì •í™•ë„ {knn_result }%')
  ``` <br/><br/>
  
 - _**SVM Model**_ <br/> 
    - _SVM works by mapping data to a high-dimensional feature space so that data points can be categorized, even when the data are not otherwise linearly separable. A separator between the categories is found, then the data are transformed in such a way that the separator could be drawn as a hyperplane._<br/><br/> 
  
    ```
    # SVM ë¶„ë¥˜ë¥¼ ìœ„í•´ svm ì„¤ì •
    from sklearn import svm
    # SVM ë¶„ë¥˜ ëª¨í˜•í™” : ì„ í˜•ë¶„ë¦¬
    clf = svm.SVC(kernel='linear')
    # ëª¨í˜• í•™ìŠµ
    clf.fit(x_train_std, y_train)
    ```
    ```
    # SVM í•™ìŠµëœ ëª¨í˜•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ë¥˜í–ˆì„ë•Œì˜ ì •í™•ë„ 
    svm_test_acuaracy = clf.score(x_test_std, y_test)
    svm_result = round(svm_test_acuaracy*100, 2)
    print(f'SVM ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ë¶„ë¥˜ ì •í™•ë„ {svm_result}%')
    ``` 
    
  - _**C5.0 Model**_ <br/> 
    - _C5.0 works by splitting the sample based on the field that provides the maximum information gain . Each sub-sample defined by the first split is then split again, usually based on a different field, and the process repeats until the subsamples cannot be split any further._ <br/><br/>
  
    ``` 
    # C5.0 ë¶„ë¥˜ë¥¼ ìœ„í•´ tree ì„¤ì •
    # C5.0 í•™ìŠµ í›„ì— í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´
    from sklearn import tree
    from sklearn.metrics import accuracy_score
    # C5.0 ëª¨í˜•í™” : ì…ë ¥ë³€ìˆ˜ì— ì˜í•œ ëª©í‘œë³€ìˆ˜ì˜ í‰ê°€ì§€í‘œëŠ” 'entropy'ë¡œ ì„¤ì •
    clf = tree.DecisionTreeClassifier(criterion="entropy")
    # ëª¨í˜• í•™ìŠµ 
    # ì°¸ê³ ë¡œ C5.0 ì•Œê³ ë¦¬ì¦˜ì€ í•™ìŠµ ë°ì´í„°ì˜ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ì„ í•  í•„ìš”ê°€ ì—†ìŒ 
    # ë°˜ëŒ€ë¡œ, KNN ì•Œê³ ë¦¬ì¦˜ & SVM ì•Œê³ ë¦¬ì¦˜ì€ í•™ìŠµ ë°ì´í„°ì˜ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”)
    clf.fit(x_train, y_train)
    ```
    <img src="https://github.com/qortmdgh4141/Classifying_Wines_by_Quality_Using_Machine_Learning/blob/main/image/tree_graph.png?raw=true"  width="580" > <br/>
    ```
    # C5.0 í•™ìŠµëœ ëª¨í˜•ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¶„ë¥˜í–ˆì„ë•Œì˜ ì •í™•ë„ 
    # ì°¸ê³ ë¡œ C5.0 ì•Œê³ ë¦¬ì¦˜ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ì„ í•  í•„ìš”ê°€ ì—†ìŒ 
    # ë°˜ëŒ€ë¡œ, KNN ì•Œê³ ë¦¬ì¦˜ & SVM ì•Œê³ ë¦¬ì¦˜ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ì´ í•„ìš”)
    y_pred = clf.predict(x_test)
    c5_0_result = round(accuracy_score(y_test, y_pred)*100, 2)
    print(f'C5.0 ì•Œê³ ë¦¬ì¦˜ì„ ì´ìš©í•œ ë¶„ë¥˜ ì •í™•ë„ {c5_0_result}%')
    ```
    <br/>

### 4. &nbsp; Research Results <br/><br/>   


  ```
  def gradientbars(bars, cmap_list):
      # cmap ê°€ì¤‘ì¹˜ ì„¤ì •
      grad = np.atleast_2d(np.linspace(0,1,256)).T
      # í”Œë¡¯ ì˜ì—­ ì¬ì„¤ì •
      ax = bars[0].axes
      lim = ax.get_xlim()+ax.get_ylim()
      ax.axis(lim)
      # ê° ë§‰ëŒ€ì— ìƒ‰ ì…íˆê¸°
      max = 0
      for i, bar in enumerate(bars):
          bar.set_facecolor("none")
          x,y = bar.get_xy()
          w, h = bar.get_width(), bar.get_height()
          ax.imshow(grad, extent=[x,x+w,y,y+h], aspect="auto", cmap=cmap_list[i])

          plt.text(x+w/2.0+0.015, h+0.7, "{}%".format(h), fontsize=14, ha='center', va='bottom')

  fig, ax = plt.subplots(figsize=(8,8))
  df = pd.DataFrame({'Model':['KNN', 'SVM', 'C5.0'], 'Accuracy':[knn_result, svm_result, c5_0_result]})
  cmap_color = ['viridis_r', 'YlOrRd', 'viridis_r']
  gradientbars(ax.bar(df.Model, df.Accuracy), cmap_color)

  plt.title("     < Comparison of classification accuracy of 3 models >     \n", fontsize=18)
  plt.ylabel('Accuracy', fontsize=16)
  plt.ylim([0, 100])
  plt.xticks(fontsize=16)
  plt.show()
  ```
   <br/>

<p align="center">
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<img src="https://github.com/qortmdgh4141/Classifying_Wines_by_Quality_Using_Machine_Learning/blob/main/image/bar_graph.png?raw=true" alt="bar_graph" width="640" >&nbsp;&nbsp;&nbsp;&nbsp;
</p> <br/> <br/> <br/>

--------------------------
### ğŸ’» S/W Development Environment
<p>
  <img src="https://img.shields.io/badge/Windows 10-0078D6?style=flat-square&logo=Windows&logoColor=white"/>
  <img src="https://img.shields.io/badge/Google Colab-black?style=flat-square&logo=Google Colab&logoColor=yellow"/>
  <img src="https://img.shields.io/badge/Python-3776AB?style=flat-square&logo=Python&logoColor=white"/>
</p>
<p>
  <img src="https://img.shields.io/badge/scikit learn-blue?style=flat-square&logo=scikitlearn&logoColor=F7931E"/>
  <img src="https://img.shields.io/badge/Numpy-013243?style=flat-square&logo=Numpy&logoColor=blue"/>
</p>

### ğŸ’¾ Dataset used in the project
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Wine Recognition Dataset
